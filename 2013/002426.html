<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Sorcerer-admins] summary of changes that require SA	interaction:
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/sorcerer-admins/2013/index.html" >
   <LINK REL="made" HREF="mailto:sorcerer-admins%40lists.berlios.de?Subject=Re%3A%20%5BSorcerer-admins%5D%20summary%20of%20changes%20that%20require%20SA%0A%09interaction%3A&In-Reply-To=%3CCAER78vR8Jqo4-rdt4DXjOvF0mwPcW9uGdnrP0K7yp87ry%2Bz8uA%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002425.html">
   <LINK REL="Next"  HREF="002427.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Sorcerer-admins] summary of changes that require SA	interaction:</H1>
    <B>Evert Vorster</B> 
    <A HREF="mailto:sorcerer-admins%40lists.berlios.de?Subject=Re%3A%20%5BSorcerer-admins%5D%20summary%20of%20changes%20that%20require%20SA%0A%09interaction%3A&In-Reply-To=%3CCAER78vR8Jqo4-rdt4DXjOvF0mwPcW9uGdnrP0K7yp87ry%2Bz8uA%40mail.gmail.com%3E"
       TITLE="[Sorcerer-admins] summary of changes that require SA	interaction:">evorster at gmail.com
       </A><BR>
    <I>Wed May  1 11:48:37 CEST 2013</I>
    <P><UL>
        <LI>Previous message: <A HREF="002425.html">[Sorcerer-admins] summary of changes that require SA interaction:
</A></li>
        <LI>Next message: <A HREF="002427.html">[Sorcerer-admins] summary of changes that require SA	interaction:
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2426">[ date ]</a>
              <a href="thread.html#2426">[ thread ]</a>
              <a href="subject.html#2426">[ subject ]</a>
              <a href="author.html#2426">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Now that I am reading through this...

So far every change to Sorcery was an optional one. When the kernel became
modular, I used a monolithic kernel for a long time, and I am still tempted
to roll my own every now and then...
There is a similar story with tomoyo and paradigms...

I manually back up my root filesystem, and restoration is a simple restore
from a tar file.

So, since I use Sorcerer as a desktop OS, and don't really need multi root
support, why do I have to jump through all these hoops?

How will all of this affect my partitioning?

One complication I can see straight off the bat is that I now have multiple
roots to back up and keep up to date, where previously it was only one.

So, I only had one root before, where are the others going to come from?
How does one log in to the various roots?

Lastly, &quot;main&quot; for a name of the initial root filesystem might be a little
confusing still.

May I suggest &quot;core&quot; for the initial filesystem, &quot;desktop&quot; or &quot;user&quot; for
the filesystem a user could be running graphical applications on, and then
&quot;www&quot; and &quot;ftp&quot; and &quot;testing&quot; as suggestions for the other filesystems.

One serious concern from my side is also the space used. My current root
uses 75% of the partition it's mounted on, and it's size is pretty
constant. The size of the partition was picked specifically to have the fs
filled to it's most efficient level. If I wanted to test some games, etc, I
would need that much space again, as I need literally everything that is on
my desktop root to be there to test...
So, for testing new spells, paradigm might be a better choice. However,
that does not protect me from aliens.


As a way forward for me....

I will probably try to keep my core root as small as possible, and only
chroot to a desktop root, which will probably contain everything I have
now, and where my users will be playing.  I might investigate unionfs for
my particular needs.

I have some software installed that is not tracked by Sorcerer, and would
have loved to be able to just load that software on request, and leave my
Sorcerer installation pristine. In an ideal world, a spell would have been
able to take care of all this, but this is not an ideal world.... I was
hunting down music duplicates with the beets music manager.... and
installing that requires the pip utility, which downloads binaries to your
root...

Kyle, don't take this the wrong way. Having multiple root support is a good
thing, as it does provide an extra layer of security for web-facing
servers. A desktop user like me just does not find a lot of utility from
it, unfortunately.

-Evert-





On 20 April 2013 21:07, Kyle Sallee &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">kyle.sallee at gmail.com</A>&gt; wrote:

&gt;<i> Reading this summary of changes will help SAs make the required changes
</I>&gt;<i> so that boxes function correctly after updating and rebooting.
</I>&gt;<i>
</I>&gt;<i> First and foremost the most important change is that
</I>&gt;<i> all root file systems will run in an unshared
</I>&gt;<i> IPC MNT NET PID UTS namespace.
</I>&gt;<i> This has many effects.
</I>&gt;<i>
</I>&gt;<i> Each root file system will have it's own hostname,
</I>&gt;<i> which will be referred to below as rootname.
</I>&gt;<i>
</I>&gt;<i> PIDs inside a root file system will have their own number system
</I>&gt;<i> and also different numbers when viewed from rootf's namespace.
</I>&gt;<i> Processes running inside an unshared PID namespace of a root file system
</I>&gt;<i> will not see the PIDs that are running on rootfs nor the PIDs that are
</I>&gt;<i> running
</I>&gt;<i> on other root filesystems.
</I>&gt;<i> Consequently, the unshared PID namespace creates an effect as if
</I>&gt;<i> each root file system was it's own virtual machine.
</I>&gt;<i>
</I>&gt;<i> The unshared network namespace for rootfs and root file systems differ.
</I>&gt;<i> The network adapters in rootfs' net namespace will stay there
</I>&gt;<i> and not be visible from each root file system's unshared net namespace.
</I>&gt;<i> A VETH, virtual ethernet adapter is created.
</I>&gt;<i> Creating 1 veth creates 2 adapters that are linked like a tunnel.
</I>&gt;<i> One VETH for each root file system stays in rootfs' net namespace.
</I>&gt;<i> The other VETH goes into the root file system's unshared net namespace.
</I>&gt;<i> From the point of view of each root file system,
</I>&gt;<i> each root file system will have it's own unique VETH adapter.
</I>&gt;<i>
</I>&gt;<i> The VETH adapter in a root file system can be assigned a static IP address.
</I>&gt;<i> Doing so requires writing the IP address followed by the FQDN of the
</I>&gt;<i> root file system
</I>&gt;<i> in /etc/hosts of that root file system.
</I>&gt;<i> If that is done then the VETH inside the root file system is assigned
</I>&gt;<i> the static IP address and a route is created in the rootfs' network
</I>&gt;<i> namespce
</I>&gt;<i> to direct packets destined for that IP address to the correct VETH adapter.
</I>&gt;<i> Assigning a static IP address then allows all the ports on the VETH
</I>&gt;<i> instant accessibility.
</I>&gt;<i>
</I>&gt;<i> Regardless of whether the VETH is assigned a static IP address
</I>&gt;<i> it will be assigned an IP address from the 169.254.2.0-255 range.
</I>&gt;<i> Those IP addresses are local to each box and therefore not routed
</I>&gt;<i> across the LAN.
</I>&gt;<i> However, ports for the network adapters in the rootfs name space
</I>&gt;<i> can be forwarded to the VETH in a root file system.
</I>&gt;<i> Therefore, FQDN www.happy.net which
</I>&gt;<i> is the root file system mounted as /media/root/www/
</I>&gt;<i> can have port 80 of the network adapter forwarded to port 80 of it's VETH.
</I>&gt;<i> And that way it can provide web pages.
</I>&gt;<i>
</I>&gt;<i> All packets leaving the box receive a SNAT translation
</I>&gt;<i> which allows each root file system to connect to the outside world
</I>&gt;<i> and have packets properly return to the network adapter in the rootfs
</I>&gt;<i> and then sent to the VETH.
</I>&gt;<i> Without the SNAT then the packets would not correctly route back
</I>&gt;<i> to the 169.254.2.0-255 IP addresses.
</I>&gt;<i>
</I>&gt;<i> So in summary, the rootfs network name space acts like a NATing firewall.
</I>&gt;<i> DHCP assignment of IP address within root file systems will not work,
</I>&gt;<i> unless the SA discovers the IP assigned and uses the /sbin/ip tool
</I>&gt;<i> to create a route to the proper VETH.
</I>&gt;<i> The 169.254.2.0 IP address are just as good as DHCP assigned
</I>&gt;<i> when the intention is to NAT a port offering a service.
</I>&gt;<i> And the static assignment of IP address will grant access to
</I>&gt;<i> all the ports without doing the NAT port forwarding.
</I>&gt;<i>
</I>&gt;<i> As previously mentioned ports can be forwarded.
</I>&gt;<i> Two files and two init-scripts control this.
</I>&gt;<i> /etc/init.d/port.forward reads /etc/port.forward
</I>&gt;<i> /etc/port.forward contains lines such as:
</I>&gt;<i>
</I>&gt;<i> www.happy.com 80 80
</I>&gt;<i>
</I>&gt;<i> The first field will be a FQDN.
</I>&gt;<i> The FQDN will be the same as the content of /etc/rootname in the root
</I>&gt;<i> file system.
</I>&gt;<i> The port or port ranges are specified using the same notation as allowed
</I>&gt;<i> with the /sbin/ip tools.  Therefore, a range of ports can be specified
</I>&gt;<i> if desired.
</I>&gt;<i> Or the port specification can use service name such as:
</I>&gt;<i>
</I>&gt;<i> www.happy.com www www
</I>&gt;<i> www.happy.com ssh ssh
</I>&gt;<i>
</I>&gt;<i> The other configuration file and init-scripts are
</I>&gt;<i> /etc/port.receive /etc/init.d/port.receive
</I>&gt;<i> The configuration format for /etc/port.receive is nearly the same
</I>&gt;<i> as the configuration syntax for /etc/port.forward except
</I>&gt;<i> the no FQDN field exists.
</I>&gt;<i> That is because /etc/port.receive is a file read from each root file system
</I>&gt;<i> where the SA of that root file system can request that those
</I>&gt;<i> ports be forwarded to it's VETH during boot.
</I>&gt;<i> If allowing that is LAX security then the SA can disable it
</I>&gt;<i> by executing the command
</I>&gt;<i> chmod 400 /etc/init.d/port.receive
</I>&gt;<i> in the root file system that is used to create the initramfs images
</I>&gt;<i> that are extracted onto rootfs during boot.
</I>&gt;<i>
</I>&gt;<i> The root file system used to create the initramfs images used during boot
</I>&gt;<i> is typically the first root file system specified in /etc/fstab.rootfs
</I>&gt;<i> It can have any name.
</I>&gt;<i> Initially, I was calling it real.
</I>&gt;<i> Now I call it main.
</I>&gt;<i> So each box will have a root file system mounted on /media/root/main/
</I>&gt;<i> And that root file system will have a FQDN that is specified in
</I>&gt;<i> /etc/rootname of it's file system.
</I>&gt;<i>
</I>&gt;<i> Sow with the FQDN specified in /etc/rootname then what is the use of
</I>&gt;<i> /etc/hostname?
</I>&gt;<i> /etc/hostname specifies the FQDN for the rootfs.
</I>&gt;<i> So the rootname is very much like hostname and in the unshared namespace
</I>&gt;<i> of each root file system each root file system receives it's own hostname
</I>&gt;<i> that is specified in file /etc/rootname
</I>&gt;<i>
</I>&gt;<i> If /etc/hostname for a box is happy.com
</I>&gt;<i> then must the FQDN for the main root file system and www root file system
</I>&gt;<i> be
</I>&gt;<i> main.happy.com and www.happy.com
</I>&gt;<i> And the answer to that is no.
</I>&gt;<i> whatever the single line content of /etc/rootname specifies
</I>&gt;<i> becomes the FQDN for the root file system.
</I>&gt;<i> Consequently, if many web servers for sites are hosted
</I>&gt;<i> then each site can have it's own root file system
</I>&gt;<i> and can have it's own FQDN.
</I>&gt;<i> Nothing about the mount point of a root file system
</I>&gt;<i> must correlate with the FQDN.
</I>&gt;<i> Whenever possible, making them similar could help avoid confusion.
</I>&gt;<i>
</I>&gt;<i> Obviously, these changes that make use of VETH,
</I>&gt;<i> NATing firewall rules, unshared name space and more
</I>&gt;<i> are part of the overall transition to multiroot.
</I>&gt;<i> Consequently, what follows is a brief discussion an suggestions
</I>&gt;<i> for the names for root file systems and their role.
</I>&gt;<i>
</I>&gt;<i> /media/root/main/ root file system can be populated
</I>&gt;<i> with only what is required in order to update it it
</I>&gt;<i> and the software that is required for booting.
</I>&gt;<i> The initramfs images used to populate the rootfs can come from main.
</I>&gt;<i> Keeping the file system trimmed helps keep the rootfs smaller.
</I>&gt;<i> Less will be swapped.
</I>&gt;<i> Running fewer network services from rootfs is advised
</I>&gt;<i> because the rootfs is acting as firewall.
</I>&gt;<i> So in a weird sort of way the main file system has a strong
</I>&gt;<i> correlation with the purpose of a /boot/ file system.
</I>&gt;<i> However, the portions of the main root file system that enter
</I>&gt;<i> into the initramfs images are still mostly from /bin/ /etc/ /lib/ /lib64/
</I>&gt;<i> /sbin/
</I>&gt;<i> and not much from the /usr/ /var/ /opt/ and other areas.
</I>&gt;<i>
</I>&gt;<i> /media/root/work/ is a fine mount point for the root file system
</I>&gt;<i> that contains workstation related files, to include xorg KDE firefox
</I>&gt;<i> and all the fun software approved for work use.
</I>&gt;<i>
</I>&gt;<i> /media/root/user/ could be the mount point for a root file system
</I>&gt;<i> on a mobile computer issued to employees.
</I>&gt;<i> Employees can be granted the root account password for /media/root/user/
</I>&gt;<i> and be allowed to install, upgrade, maintain, uninstall the software that
</I>&gt;<i> would be used for personal use.
</I>&gt;<i> Because the software for personal use and work use exists
</I>&gt;<i> in two separate root file systems
</I>&gt;<i> no conflicts or problems should result from what the user wants
</I>&gt;<i> to install and run in the /media/root/user/ file system.
</I>&gt;<i>
</I>&gt;<i> So why allow the user his own root file system on a company owned computer?
</I>&gt;<i> Obviously, the user would run a qemu virtual machine
</I>&gt;<i> in his home directory if not provided with the root account password
</I>&gt;<i> for his box.
</I>&gt;<i> By running a virtual machine the user gains that root level ability
</I>&gt;<i> within his virtual machine, but at the cost of performance,
</I>&gt;<i> perhaps also 3D accelerated graphics, and significant RAM overhead.
</I>&gt;<i>
</I>&gt;<i> In contrast a user with his own root file system gains the file system
</I>&gt;<i> isolation for his own processes, PID isolation, network adapter isolation,
</I>&gt;<i> accelerated graphics, and as much access to RAM as allowed by control
</I>&gt;<i> groups
</I>&gt;<i> without the RAM overhead that would occur
</I>&gt;<i> for running a virtual computer and 2 linux kernels!
</I>&gt;<i> And running a single linux kernel provides more efficient caching and
</I>&gt;<i> swapping,
</I>&gt;<i> way better than can be achieved using virtual computers with multiple
</I>&gt;<i> linux kernels
</I>&gt;<i> and memory ballooning.
</I>&gt;<i>
</I>&gt;<i> /media/root/www/
</I>&gt;<i> Is the obvious location for a box to run it's web server.
</I>&gt;<i> Having nothing else there except web server related files
</I>&gt;<i> and what is required to update the root file system
</I>&gt;<i> makes it almost like a running the web server chrooted.
</I>&gt;<i> And in actuality that is what is happening.  :)
</I>&gt;<i>
</I>&gt;<i> Web servers do not typically run as UID 0.
</I>&gt;<i> Exploiting a bug not create a compromise
</I>&gt;<i> that allows escalation to EUID 0.
</I>&gt;<i> However, to truly prevent damage from nasty interlopers;
</I>&gt;<i> immune-security-tomoyo has been revised for multi-root support.
</I>&gt;<i> It will keep interlopers locked in the domain box, typically either
</I>&gt;<i> the user or limited domains.
</I>&gt;<i> Those domains will allow files to be viewed according to one's EUID and
</I>&gt;<i> EGID.
</I>&gt;<i> However, modification of files in system owned areas such as:
</I>&gt;<i> /bin/ /etc/ /lib/ /lib64/ /opt/ /sbin/ /usr/ /var/ are prohibited.
</I>&gt;<i> Such an interloper must also be able to authenticate with a proper password
</I>&gt;<i> to escape into the freedom domain before gaining
</I>&gt;<i> the freedom required to wreak havoc.
</I>&gt;<i> Consequently, immune-security-tomoyo helps enforce the traditional
</I>&gt;<i> linux security model.
</I>&gt;<i>
</I>&gt;<i> If only one computer was purchased
</I>&gt;<i> and that computer was intended to host several Sorcerer computers
</I>&gt;<i> then that computer will also have a /media/root/dev/ root file system.
</I>&gt;<i> The /media/root/dev/ root file system can contain a loadout of software
</I>&gt;<i> that is installed on all of the other root file systems.
</I>&gt;<i> And /media/root/dev/ can be updated before the other root file systems.
</I>&gt;<i> That way the update can be observed, the services can be observed,
</I>&gt;<i> and problems can be discovered and expected or circumvented
</I>&gt;<i> before performing updates on the other root file systems.
</I>&gt;<i>
</I>&gt;<i> So in summary, the root file systems mounted on /media/root/
</I>&gt;<i> should be granted a name that easily identifies the role.
</I>&gt;<i> Each root file system will act like a virtual computer without
</I>&gt;<i> actually requiring virtual computer software and without
</I>&gt;<i> booting additional linux kernels.
</I>&gt;<i> Up to 255 root file systems are supported.
</I>&gt;<i> But I would not recommend doing more than 62.
</I>&gt;<i> The linux kernel provides 64 virtual consoles.
</I>&gt;<i> Each of the root file systems will run a logind
</I>&gt;<i> that will select the first available virtual console
</I>&gt;<i> and present a prompt which will run the login program
</I>&gt;<i> when a key is pressed.
</I>&gt;<i> So if running more than 62 root file systems
</I>&gt;<i> then some of those root file systems will require
</I>&gt;<i> chmod 400 /etc/init.d/logind or the kernel source will require
</I>&gt;<i> modification for creating more than 64 virtual consoles.
</I>&gt;<i> Either way would work.
</I>&gt;<i>
</I>&gt;<i> The control group implementation is partially aware of PID name spaces.
</I>&gt;<i> To simplify any confusion, each root file system
</I>&gt;<i> receives it's own control group hierarchy.
</I>&gt;<i> The rootfs file system is at the top of the hierarchy and can peer
</I>&gt;<i> into the control group hierarchy of each root file system.
</I>&gt;<i> Therefore, controls can be set for the rootfs' /sys/fs/cgroup/
</I>&gt;<i> that limit the amount of processes and RAM and swap
</I>&gt;<i> that is provided to each root file system.
</I>&gt;<i> So if a box had 8 processors and 6 root file systems.
</I>&gt;<i> Then some of those root file systems might be granted dedicated
</I>&gt;<i> use of only a single processor if the SA desires.
</I>&gt;<i> However, by default no limitations are created.
</I>&gt;<i> Therefore, each of the root file systems can equally compete
</I>&gt;<i> for the processing power of all 8 processors.
</I>&gt;<i>
</I>&gt;<i> As an example let us assume that a SA is deploying a company mobile
</I>&gt;<i> computer.
</I>&gt;<i> So he creates
</I>&gt;<i> /media/root/{main,work,user,dev}
</I>&gt;<i> He configures the dev root file system to automatically update.
</I>&gt;<i> But he does not want it to SAP the user's processing power.
</I>&gt;<i> So he grants only one processor and to /media/root/dev/
</I>&gt;<i> and sets the cpu.shares so that /media/root/dev/ receives
</I>&gt;<i> 1/64th the total processing power when competing with other root file
</I>&gt;<i> systems.
</I>&gt;<i> dev could require a while to update.
</I>&gt;<i> But the resource usage will be so slightly
</I>&gt;<i> that the user will be unaware of any activity
</I>&gt;<i> that is automatically happening on /media/root/dev/
</I>&gt;<i>
</I>&gt;<i> For added security in the above example
</I>&gt;<i> the 3 or all four of root file systems can be created
</I>&gt;<i> on LUKS encrypted devices.
</I>&gt;<i> Therefore, during boot selection of which file systems are mounted
</I>&gt;<i> depends on giving the proper password.
</I>&gt;<i> The user can be provided with LUKS passphrases that unlock
</I>&gt;<i> the encryption key for /media/root/user/ and /media/root/work/
</I>&gt;<i> Or those passphrases can even be the same.
</I>&gt;<i> And by having different passphrases for /media/root/main/
</I>&gt;<i> and /media/root/dev/
</I>&gt;<i> then those file systems will mount only when the SA can enter
</I>&gt;<i> those pass phrases during boot.
</I>&gt;<i> However, if /media/root/dev/ is not on a LUKS encrypted device
</I>&gt;<i> then it can be mounted on each boot and run that way.
</I>&gt;<i>
</I>&gt;<i> Eventually, the time for updates arrives.
</I>&gt;<i> The user hands the mobile computer to the SA.
</I>&gt;<i> The SA examines the event logs in /media/root/dev/
</I>&gt;<i> and examines the services to see if they are functioning.
</I>&gt;<i> If it all looks good then the SA can begin the updates
</I>&gt;<i> on /media/root/{main,user,work}/
</I>&gt;<i> Slick right?
</I>&gt;<i> The sources are already downloaded.
</I>&gt;<i> And ccache archive tarballs could already exist.
</I>&gt;<i> And those can be shared among root file systems
</I>&gt;<i> by using the bind command.
</I>&gt;<i>
</I>&gt;<i> After specifying root file systems to mount in /etc/fstab.rootfs
</I>&gt;<i> additional lines can provide some convenient binds such as:
</I>&gt;<i>
</I>&gt;<i> /media/root/dev/aux/can /media/root/main/aux/can none bind 0 0
</I>&gt;<i> /media/root/dev/aux/can /media/root/user/aux/can none bind 0 0
</I>&gt;<i> /media/root/dev/aux/can /media/root/work/aux/can none bind 0 0
</I>&gt;<i>
</I>&gt;<i> Now all root file systems share the same view of /aux/can/
</I>&gt;<i> That helps prevent the waste of disk space.
</I>&gt;<i>
</I>&gt;<i> I apologize for the complexity introduced by the changes.
</I>&gt;<i> I have made the implementations with nearly the least complexity as
</I>&gt;<i> possible
</I>&gt;<i> that still achieve full utilization of unshared namespace.
</I>&gt;<i> This way computers can gain many of the best features of running
</I>&gt;<i> virtual computers,
</I>&gt;<i> with none of the drawbacks nor resource overhead.
</I>&gt;<i>
</I>&gt;<i> Some SAs might expect that this sort of configuration
</I>&gt;<i> has been available for years with other POSIX.
</I>&gt;<i> However, I doubt it.
</I>&gt;<i> To use the unshared namespace boxes must boot with these new linux-* spells
</I>&gt;<i> soon to be available in the svn grimoire, must be upgraded to glibc
</I>&gt;<i> version 2.17
</I>&gt;<i> and must be upgraded to util-linux version 2.23-rc1
</I>&gt;<i> Yes, that is the release candidate from the developmental branch of
</I>&gt;<i> util-linux.
</I>&gt;<i> However, it provides the utilities that are required.
</I>&gt;<i> Failure to do all the required upgrades might create a box that can boot,
</I>&gt;<i> but will definitely lack the unshared namespace and consequently
</I>&gt;<i> maybe not all things will work as expected.
</I>&gt;<i> My recommendation is don't try don't see.
</I>&gt;<i> Be certain that the updates pass.
</I>&gt;<i>
</I>&gt;<i> I have not yet completed the code for upgrading paradigm support to
</I>&gt;<i> unshared namespace.
</I>&gt;<i> I will eventually.
</I>&gt;<i> So paradigm support will initially be broke for a short while.
</I>&gt;<i> Paradigms can be created and shifted and that is about it.
</I>&gt;<i> Paradigms are still an excellent way to backup a root file system
</I>&gt;<i> before upgrading it.
</I>&gt;<i> That way it could be more quickly and effectively downgraded
</I>&gt;<i> to the pre-update state if desired.
</I>&gt;<i>
</I>&gt;<i> Archive support is deprecated.
</I>&gt;<i> /aux/can/archive/ is a junk directory now.
</I>&gt;<i> If not desired then it can be deleted after the update.
</I>&gt;<i> When grep -q ARCHIVE_CACHE /etc/sorcery/config
</I>&gt;<i> has no output then the /aux/can/archive/ is nothing but an ancient
</I>&gt;<i> reminder.
</I>&gt;<i> The best abilities of archives is now available using paradigms.
</I>&gt;<i> And paradigms provide a complete hard linked mirror for
</I>&gt;<i> how a file system was loaded at a given time.
</I>&gt;<i> That is far superior to a collection of archives.
</I>&gt;<i>
</I>&gt;<i> Extraction of a program or library from archive was never a guarantee
</I>&gt;<i> that the program or library would be usable.
</I>&gt;<i> The files from the archive could be sufficiently old that the available
</I>&gt;<i> sonames for installed libraries differ from what the software in the
</I>&gt;<i> archive requires.
</I>&gt;<i> Consequently, in those situations those spells must be moiled.
</I>&gt;<i> And nothing indicated when an archive went from useful to junk.
</I>&gt;<i> At best they might be used to immediately roll back the version
</I>&gt;<i> of software that was updated just recently.
</I>&gt;<i> And then because of the interdependency of ELF libraries
</I>&gt;<i> perhaps installed libraries would also require reversion to a previous
</I>&gt;<i> archive.
</I>&gt;<i> It was too tedious for most SAs to do.
</I>&gt;<i> The archive cache wasted disk space and processing cycles.
</I>&gt;<i> I am glad to see it go.
</I>&gt;<i>
</I>&gt;<i> The changes described above will become available sometime
</I>&gt;<i> between now and the end of April.
</I>&gt;<i> Testing looks fine, the situation looks good, everything that should
</I>&gt;<i> be working seems to be working.
</I>&gt;<i> However, any mistake in the implementation could cause a significant
</I>&gt;<i> inconvenience.
</I>&gt;<i>
</I>&gt;<i> For example, until I created the SNAT firewall rules everything about
</I>&gt;<i> the root file system on a box
</I>&gt;<i> seemed to work properly, except that it could not download files via
</I>&gt;<i> FTP, HTTP, nor SSH out, etc...
</I>&gt;<i> And the SNAT rule fixed that.
</I>&gt;<i> However, if SAs updated their boxes, rebooted, logged in, and discovered
</I>&gt;<i> that
</I>&gt;<i> browsing the WWW, doing email, and more was not working
</I>&gt;<i> then would that not be a tremendous inconvenience?
</I>&gt;<i>
</I>&gt;<i> As a result of the sweeping changes made to init-scripts, linux
</I>&gt;<i> related spells, spells that install linux modules,
</I>&gt;<i> and much more; I have been testing my checked out copy of the svn
</I>&gt;<i> grimoire and sitting on the changes
</I>&gt;<i> rather than trying to commit pieces.
</I>&gt;<i> If I tried to commit pieces, or individual spells and their $SPELL.d/
</I>&gt;<i> directories
</I>&gt;<i> then I might make a mistake and commit the entire grimoire to svn,
</I>&gt;<i> then moments later berlios.de would have one of it's outages
</I>&gt;<i> and then I would be unable to commit fixes for the bugs
</I>&gt;<i> and the rolled grimoire would be a mess if installed on a box.
</I>&gt;<i> Because berlios.de might only give me one chance at a commit
</I>&gt;<i> I must make absolutely certain that the grimoire that goes out
</I>&gt;<i> is thoroughly tested and the spells are excellent.
</I>&gt;<i>
</I>&gt;<i> I am uncertain when I last committed to svn grimoire.
</I>&gt;<i> Probably, I have been sitting on changes for about a month.
</I>&gt;<i> Maybe some people expected that I went on vacation?
</I>&gt;<i> Anyone who conjectures that Modern Magic became abandon-ware think AGAIN!
</I>&gt;<i> Just because I work nearly incessantly for a month
</I>&gt;<i> making all aspects commensurate
</I>&gt;<i> with the awesome new features in the linux kernel
</I>&gt;<i> does not mean that I abandoned the POSIX.
</I>&gt;<i> I apologize for the bugs that had to be endured
</I>&gt;<i> and the updates that had to be sacrificed
</I>&gt;<i> while I designed and implemented the new code.
</I>&gt;<i>
</I>&gt;<i> BTW, after computers are updated,
</I>&gt;<i> please do not reboot them unattended.
</I>&gt;<i> A possibility exists that the old init-scripts
</I>&gt;<i> are sufficiently incompatible with the new init-scripts
</I>&gt;<i> that boxes will not be run the command:
</I>&gt;<i> reboot -d -f -i
</I>&gt;<i> Please look for that.
</I>&gt;<i>
</I>&gt;<i> Until the updates arrive in the rolled grimoire
</I>&gt;<i> each SA should update the /etc/rootname for each file system.
</I>&gt;<i> Remember that /etc/rootname for each file system should be unique.
</I>&gt;<i> And /etc/rootname and /etc/hostname should not be the same.
</I>&gt;<i> Also edit /etc/fstab.rootfs for the main root file system.
</I>&gt;<i> Remember that mount points in it for root file systems start with
</I>&gt;<i> /media/root/
</I>&gt;<i> An /etc/fstab.rootfs will be required.
</I>&gt;<i> Then edit /etc/port.forward
</I>&gt;<i> Specify what should be forwarded.
</I>&gt;<i> At the very least a line such as:
</I>&gt;<i>
</I>&gt;<i> main.something.tld ssh ssh
</I>&gt;<i> will allow port 22 to be forwarded into the real root file system.
</I>&gt;<i> The sshd running on rootfs will still listen on port 30.
</I>&gt;<i>
</I>&gt;<i> After the first reboot with the new update
</I>&gt;<i> probably some error messages might scroll concerning init
</I>&gt;<i> and the starting of gettys and such.
</I>&gt;<i> The solution is to comment out the lines in /etc/inittab
</I>&gt;<i> of the main root file system so that gettys are not started.
</I>&gt;<i> logind will provide the ability to log in and all root file systems
</I>&gt;<i> including the rootfs.
</I>&gt;<i>
</I>&gt;<i> Finally, for some unknown reason /dev/tty1
</I>&gt;<i> the very first virtual console does not become available
</I>&gt;<i> even after no process has /dev/tty1 as it's controlling tty.
</I>&gt;<i> So it will have the scroll from sysinit and runlevel startup on it.
</I>&gt;<i> But it will not prompt for login unless a getty is specified for it in
</I>&gt;<i> /etc/inittab
</I>&gt;<i> Consequently, use alt right arrow to switch to the next virtual console
</I>&gt;<i> in the list of active virtual consoles.
</I>&gt;<i> Keep switching until the virtual console is discovered
</I>&gt;<i> that can provide a login for the desired root file system.
</I>&gt;<i> The dynamic allocation of virtual consoles for root file system login
</I>&gt;<i> means that a login for a root file system might not
</I>&gt;<i> always appear on the same virtual console as it did before.
</I>&gt;<i> If SysRq K is used to kill the login prompt on the virtual console
</I>&gt;<i> then it will appear on some other virtual console.
</I>&gt;<i> That makes it a bit tricky to securely kill.
</I>&gt;<i> However, kill it once, find it's new virtual console, kill it there
</I>&gt;<i> and it should re-appear on it's original virtual console.
</I>&gt;<i>
</I>&gt;<i> Likewise GUI logins for multiple root file systems can appear on
</I>&gt;<i> different virtual consoles.
</I>&gt;<i> If running more than one then look around to carefully select
</I>&gt;<i> the correct GUI login prompt for the desired root file system.
</I>&gt;<i>
</I>&gt;<i> The mount --bind thing suggested previously with multiple binding of
</I>&gt;<i> /aux/can/
</I>&gt;<i> can also be used to mount --bind the same /home in several root file
</I>&gt;<i> systems.
</I>&gt;<i>
</I>&gt;<i> Hopefully, that conveys everything essential.
</I>&gt;<i> Comprehension will be easier after the upgrade happens.
</I>&gt;<i> Remember that before the upgrade modify
</I>&gt;<i> /etc/rootname
</I>&gt;<i> /etc/fstab.rootfs
</I>&gt;<i> /etc/port.forward
</I>&gt;<i> and if a static IP address is desired
</I>&gt;<i> /etc/hosts
</I>&gt;<i>
</I>&gt;<i> The same static IP address can not be used for an ethernet adapter
</I>&gt;<i> as is used for the VETH inside of a name space.
</I>&gt;<i> So 2 will be required if the box had previously only one.
</I>&gt;<i> However, the IP address assigned to the ethernet adapter
</I>&gt;<i> can be non globally routable if desired.
</I>&gt;<i> And the globally routable IP address could be assigned to the VETH
</I>&gt;<i> in /media/root/main/ if desired
</I>&gt;<i> Of course forwarding desired ports makes it all work again
</I>&gt;<i> without having 2 static IP addresses.
</I>&gt;<i> So please consider that as an option.
</I>&gt;<i>
</I>&gt;<i> If confusion or questions arise from reading this
</I>&gt;<i> then please post inquires on the email list.
</I>&gt;<i> If one person wondered something
</I>&gt;<i> then probably many more people wondered the same.
</I>&gt;<i> That way Q&amp;A can be handled as follow up
</I>&gt;<i> and everyone's understanding can become sufficient
</I>&gt;<i> for acclimating to the changes.
</I>&gt;<i> And that alone is another good reason to allow a week
</I>&gt;<i> for discussion before I commit the changes to svn.
</I>&gt;<i>
</I>&gt;<i> I already have the new implementations running
</I>&gt;<i> on two workstations and one test box.
</I>&gt;<i> So if anyone wants me to test if something is possible
</I>&gt;<i> then I can test it and report the results.
</I>&gt;<i>
</I>&gt;<i> For those wanting a verbose description of unshared namespace
</I>&gt;<i> then a series of articles on LWN seems to have the most verbose
</I>&gt;<i> explanation.
</I>&gt;<i> Some documentation exists in /usr/src/linux/Documentation/
</I>&gt;<i> but it seems less than adequate for gaining comprehension.
</I>&gt;<i> The tools provided by util-linux provide manual pages.
</I>&gt;<i> Those are only helpful to someone who already understands unshared
</I>&gt;<i> namespace.
</I>&gt;<i>
</I>&gt;<i> None of the changes require that SA run the tools
</I>&gt;<i> /sbin/ip /sbin/iptables to accomplish any tasks.
</I>&gt;<i> Lore about VETH are, IP address assignment, routing, and firewalling
</I>&gt;<i> are a prudent acquisition for anyone who wants
</I>&gt;<i> to be able to debug any problem that might occur.
</I>&gt;<i>
</I>&gt;<i> Of course knowledge about control groups is useful for every SA,
</I>&gt;<i> and yet the implementation is so smooth that anyone can deploy and run a
</I>&gt;<i> box
</I>&gt;<i> without making any adjustments to control groups.
</I>&gt;<i> Hopefully, the multi-root implementation will also be that smooth.
</I>&gt;<i> If /etc/rootname /etc/port.forward /etc/hosts /etc/fstab.rootfs
</I>&gt;<i> are the only files requiring adjustment
</I>&gt;<i> then that is a very tiny cost for gaining
</I>&gt;<i> the awesome power of single kernel pseudo virtualization.
</I>&gt;<i> _______________________________________________
</I>&gt;<i> Sorcerer-admins mailing list
</I>&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">Sorcerer-admins at lists.berlios.de</A>
</I>&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">https://lists.berlios.de/mailman/listinfo/sorcerer-admins</A>
</I>&gt;<i>
</I>


-- 
Evert Vorster
Chief Observer
WG Cook
-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="https://lists.berlios.de/pipermail/sorcerer-admins/attachments/20130501/eefddeaf/attachment-0001.html">https://lists.berlios.de/pipermail/sorcerer-admins/attachments/20130501/eefddeaf/attachment-0001.html</A>&gt;
</PRE>


<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002425.html">[Sorcerer-admins] summary of changes that require SA interaction:
</A></li>
	<LI>Next message: <A HREF="002427.html">[Sorcerer-admins] summary of changes that require SA	interaction:
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2426">[ date ]</a>
              <a href="thread.html#2426">[ thread ]</a>
              <a href="subject.html#2426">[ subject ]</a>
              <a href="author.html#2426">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">More information about the Sorcerer-admins
mailing list</a><br>
</body></html>
