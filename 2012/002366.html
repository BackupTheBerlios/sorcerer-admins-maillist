<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Sorcerer-admins] cgroup modified and explained...
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/sorcerer-admins/2012/index.html" >
   <LINK REL="made" HREF="mailto:sorcerer-admins%40lists.berlios.de?Subject=Re%3A%20%5BSorcerer-admins%5D%20cgroup%20modified%20and%20explained...&In-Reply-To=%3CCAER78vS09z5HQai38AuiLNMTJETQ98mX%3Dm1uCzxFJPbp1ZA%3DTg%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002365.html">
   <LINK REL="Next"  HREF="002367.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Sorcerer-admins] cgroup modified and explained...</H1>
    <B>Evert Vorster</B> 
    <A HREF="mailto:sorcerer-admins%40lists.berlios.de?Subject=Re%3A%20%5BSorcerer-admins%5D%20cgroup%20modified%20and%20explained...&In-Reply-To=%3CCAER78vS09z5HQai38AuiLNMTJETQ98mX%3Dm1uCzxFJPbp1ZA%3DTg%40mail.gmail.com%3E"
       TITLE="[Sorcerer-admins] cgroup modified and explained...">evorster at gmail.com
       </A><BR>
    <I>Tue Aug 14 03:30:18 CEST 2012</I>
    <P><UL>
        <LI>Previous message: <A HREF="002365.html">[Sorcerer-admins] cgroup modified and explained...
</A></li>
        <LI>Next message: <A HREF="002367.html">[Sorcerer-admins] Sorcerer common install-script
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2366">[ date ]</a>
              <a href="thread.html#2366">[ thread ]</a>
              <a href="subject.html#2366">[ subject ]</a>
              <a href="author.html#2366">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Let me be the first to say that the default settings for cgroup are very
fine indeed for the largest possible set of system configs.

I will debug the alternate settings for cgroup, but that is really only to
be used on a single user system, and will be tuned as such, and users
running the alternate cgroup settings on servers or with many users will
run into issues that simply does not happen on the default setup.

Happy hacking!
-Evert-

On 13 August 2012 20:40, Kyle Sallee &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">kyle.sallee at gmail.com</A>&gt; wrote:

&gt;<i> I modified cgroup to be a bit more like cpufreq.
</I>&gt;<i> Therefore, cgroup-{affine,fast,idle,slow} were removed in favor of
</I>&gt;<i> /usr/bin/cgroup
</I>&gt;<i> It accepts parameters and provides bash completion.
</I>&gt;<i> Additionally, I checked to ensure that all aspects of it work properly.
</I>&gt;<i> And finally, they do.
</I>&gt;<i> Previously, there were some ownership/permission problems.
</I>&gt;<i>
</I>&gt;<i> This change along with the aforementioned change to cpufreq
</I>&gt;<i> will be available in the next UTC 00:00 grimoire.
</I>&gt;<i> And since most boxes typically lag a day behind,
</I>&gt;<i> expect to be able to enjoy the benefits no sooner than about 30 hours from
</I>&gt;<i> now.
</I>&gt;<i>
</I>&gt;<i> Basically, one of the final set of changes before releasing new I/Rs
</I>&gt;<i> is to revise implementations that have been awaiting revision
</I>&gt;<i> and to hunt bugs, almost any bugs.
</I>&gt;<i> That way the next I/Rs provide maximum functionality and minimal annoyance.
</I>&gt;<i>
</I>&gt;<i> For those who have not used the cgroup helper program
</I>&gt;<i> it combines a bit of the functionality
</I>&gt;<i> that was previously provided by nice and taskset
</I>&gt;<i> on a purely POSIX box without cgroups enabled.
</I>&gt;<i>
</I>&gt;<i> However, on a cgroup box, nice does not work exactly as expected
</I>&gt;<i> and that is the reason for the existence of top level control groups:
</I>&gt;<i> fast; norm; slow; idle.
</I>&gt;<i> By default users will not have access to fast unless enabled.
</I>&gt;<i> However on a quad core box user cgroups will look something like
</I>&gt;<i>
</I>&gt;<i> /sys/fs/cgroup/{norm,slow,idle}/user/$USER/{0,1,2,3}
</I>&gt;<i>
</I>&gt;<i> When a user logs in via ssh or kdm, gdm, xdm
</I>&gt;<i> then the user's processes run in
</I>&gt;<i> /sys/fs/cgroup/norm/user/$USER
</I>&gt;<i> However, transition from /sys/fs/group
</I>&gt;<i> for user owned processes
</I>&gt;<i> is provided when a user logs in via getty.
</I>&gt;<i>
</I>&gt;<i> Let us say that the user wants his virtual server to run slower
</I>&gt;<i> $ cpufreq slow $( pgrep qemu )
</I>&gt;<i> Easy yes?
</I>&gt;<i> What if the user wants to xz compress a file, but for minimal processing
</I>&gt;<i> usage?
</I>&gt;<i> $ xz huge.tar &amp;
</I>&gt;<i> $ cpufreq idle $( pgrep -u $USER xz )
</I>&gt;<i> And if the user also wants xz to run on a specific processor such as
</I>&gt;<i> the last one?
</I>&gt;<i> $ cpufreq affine 3 $( pgrep -u $USER xz )
</I>&gt;<i>
</I>&gt;<i> Easy.
</I>&gt;<i> But of course the user can
</I>&gt;<i> always echo user owned PIDs
</I>&gt;<i> to the tasks files owned by the user.
</I>&gt;<i> Therefore, the use of /usr/bin/cgroup
</I>&gt;<i> is optional for a SA that already understands
</I>&gt;<i> the use of control groups and the default implementation on Sorcerer.
</I>&gt;<i>
</I>&gt;<i> If expecting to use maximum processing power for a long duration on a
</I>&gt;<i> multi-user box
</I>&gt;<i> then moving one's own processes to the slow or idle hierarchy of control
</I>&gt;<i> groups
</I>&gt;<i> allows those processes to defer processing power to other processing
</I>&gt;<i> running on the box.
</I>&gt;<i> Doing so would be considered polite.
</I>&gt;<i>
</I>&gt;<i> Someone might yet wonder what happens
</I>&gt;<i> if a user starts xz in the above example and
</I>&gt;<i> then executes:
</I>&gt;<i> $ renice +19 $( pgrep -u $USER xz )
</I>&gt;<i>
</I>&gt;<i> The answer that if the scheduler will prefer to allocate
</I>&gt;<i> more processing power to the other processes that
</I>&gt;<i> are in the same control group as xz.
</I>&gt;<i> Therefore, if processing power is in short supply
</I>&gt;<i> then xz's share will be diminished relative to other
</I>&gt;<i> processes within the same control group.
</I>&gt;<i>
</I>&gt;<i> Therefore, if users bill jane and john are competing
</I>&gt;<i> for an insufficient amount of processing power.
</I>&gt;<i> And each is running their processes from
</I>&gt;<i> /sys/fs/cgroup/user/{bill,jane,john}, respectively
</I>&gt;<i> then each user will receive approximately an equal portion of processing
</I>&gt;<i> power.
</I>&gt;<i> If bill nices his xz process then bill still receives approximately
</I>&gt;<i> 1/3rd total for his processes.
</I>&gt;<i> However, if bill moves xz's PID into /sys/fs/cgroup/slow/user/bill/tasks
</I>&gt;<i> then bill's processes in /sys/fs/cgroup/norm/user/bill/tasks will run
</I>&gt;<i> faster.
</I>&gt;<i> However, his xz process will run much slower than previously.
</I>&gt;<i>
</I>&gt;<i> Perhaps the above explanation might seem confusing?
</I>&gt;<i> However, it is a very good implementation for directing processing power
</I>&gt;<i> for both uniprocess or SMP boxes regardless of whether the box has one
</I>&gt;<i> or more concurrent users.
</I>&gt;<i> And additionally, the default implementation should prevent
</I>&gt;<i> the condition of X starving for processing power.
</I>&gt;<i> Therefore, a number of tasks that contribute to a high load
</I>&gt;<i> should not impact the visual performance of X.
</I>&gt;<i> However, in extreme circumstances the user's firefox or video player
</I>&gt;<i> might be starving.
</I>&gt;<i> And that situation can be solved by using &quot;nice&quot; or the &quot;cgroup&quot;
</I>&gt;<i> utility as described above.
</I>&gt;<i>
</I>&gt;<i> Most system started services will run in cgroups such as:
</I>&gt;<i> /sys/fs/cgroup/norm/sys/$SERVICE
</I>&gt;<i> where $SERVICE is the name of the service.
</I>&gt;<i> However, some variation exists in order to make some services receive
</I>&gt;<i> preferred
</I>&gt;<i> or deferred scheduling of processing power.
</I>&gt;<i> For example, gdm, kdm, xdm will run from
</I>&gt;<i> /sys/fs/cgroup/fast/sys/{gdm,kdm,xdm}, respectively.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> For those who prefer a more simple to understand implementation that
</I>&gt;<i> works more like POSIX
</I>&gt;<i> then rejoice because Evert is designing that implementation.
</I>&gt;<i> However, lament because I am coding it, so it might have bugs.  :)
</I>&gt;<i> Evert wants for all user owned processes to always run in
</I>&gt;<i> /sys/fs/cgroup/tasks
</I>&gt;<i> Additionally, Evert wants for system started services to run in
</I>&gt;<i> /sys/fs/cgroup/$SERVICE
</I>&gt;<i> Succinctly stated, it is an implementation that sacrifices the
</I>&gt;<i> benefits of hierarchy.
</I>&gt;<i>
</I>&gt;<i> Also it does not install a /usr/bin/cgroup to assist users.
</I>&gt;<i> Users will control allocation of processing power using the &quot;nice&quot; and
</I>&gt;<i> &quot;taskset&quot; utilities.
</I>&gt;<i> And it will not maintain a fair allocation of processing power among users.
</I>&gt;<i> If john has 5 processes running full bore, and jane has one process
</I>&gt;<i> running full bore
</I>&gt;<i> then john will receive 5 times more processing power than jane.
</I>&gt;<i> It is old style where user with the most full bore processes wins.
</I>&gt;<i> Controlling the allocation of processing power among services
</I>&gt;<i> is not accomplished with nice but instead by changing
</I>&gt;<i> the value of cpu.shares for one or more service cgroups.
</I>&gt;<i>
</I>&gt;<i> After the implementation is debugged, tested, honed, revised, etc...
</I>&gt;<i> then it will be available from the spells {init-functions,cgroup}-evert
</I>&gt;<i> Until then those spells will retain the broke attribute.
</I>&gt;<i>
</I>&gt;<i> Anyone with an opinion about the default cgroup implementation or Evert's
</I>&gt;<i> is welcome to state reasons, observations, tests on the email list.
</I>&gt;<i> If we can use that information to make the cgroup implementation better
</I>&gt;<i> then we will.
</I>&gt;<i>
</I>&gt;<i> I might have already stated in a previous message,
</I>&gt;<i> but if I forgot then please be aware that
</I>&gt;<i> # /etc/init.d/cgroup configure
</I>&gt;<i> has been improved.
</I>&gt;<i> It now allows creation of a cgroup at any level instead of merely top
</I>&gt;<i> level.
</I>&gt;<i> And it allows editing of controls in any control group.
</I>&gt;<i> And it can display the control groups that are defined and initialized at
</I>&gt;<i> boot.
</I>&gt;<i> And the newest ability is to select which subsystems are enabled
</I>&gt;<i> when the cgroup file system is mounted on /sys/fs/cgroup/
</I>&gt;<i>
</I>&gt;<i> For example, why bother mounting the cpuset subsystem on a uniprocessor
</I>&gt;<i> box?
</I>&gt;<i> And the default cgroup implementation no longer mounts the blkio subsystem
</I>&gt;<i> because I changed the default io scheduler from CFQ back to deadline.
</I>&gt;<i> Therefore, if a SA wants to use blkio then the blkio subsystem can be
</I>&gt;<i> enabled
</I>&gt;<i> and the default scheduler can be selected per device or at boot by
</I>&gt;<i> kernel parameter.
</I>&gt;<i> And those with no concern for controlling memory and who do not install
</I>&gt;<i> immune-sentinel-memory
</I>&gt;<i> can for example disable the memory subsystem.
</I>&gt;<i> Those who want to use &quot;nice&quot; exclusively for scheduling processing power,
</I>&gt;<i> and still want the default cgroup implementation could choose to
</I>&gt;<i> mount the cgroup file system without the cpu subsystem.
</I>&gt;<i> The possibilities are plentiful.
</I>&gt;<i>
</I>&gt;<i> I hope that everyone will feel satisfied with the default implementation
</I>&gt;<i> or be able to suitably modify it in order to achieve desired
</I>&gt;<i> performance and functionality.
</I>&gt;<i> Of course dissatisfaction can impel change and possibly improvement.
</I>&gt;<i> Therefore, please also provide suggestions
</I>&gt;<i> for ways in which the functionality can be improved.
</I>&gt;<i> _______________________________________________
</I>&gt;<i> Sorcerer-admins mailing list
</I>&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">Sorcerer-admins at lists.berlios.de</A>
</I>&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">https://lists.berlios.de/mailman/listinfo/sorcerer-admins</A>
</I>&gt;<i>
</I>


-- 
Those who do not understand UNIX are doomed to reinvent it, poorly.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="https://lists.berlios.de/pipermail/sorcerer-admins/attachments/20120814/10bcd1fc/attachment-0001.html">https://lists.berlios.de/pipermail/sorcerer-admins/attachments/20120814/10bcd1fc/attachment-0001.html</A>&gt;
</PRE>


<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002365.html">[Sorcerer-admins] cgroup modified and explained...
</A></li>
	<LI>Next message: <A HREF="002367.html">[Sorcerer-admins] Sorcerer common install-script
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2366">[ date ]</a>
              <a href="thread.html#2366">[ thread ]</a>
              <a href="subject.html#2366">[ subject ]</a>
              <a href="author.html#2366">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">More information about the Sorcerer-admins
mailing list</a><br>
</body></html>
