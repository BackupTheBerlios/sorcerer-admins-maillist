<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Sorcerer-admins] Sorcerer common install-script
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/sorcerer-admins/2012/index.html" >
   <LINK REL="made" HREF="mailto:sorcerer-admins%40lists.berlios.de?Subject=Re%3A%20%5BSorcerer-admins%5D%20Sorcerer%20common%20install-script&In-Reply-To=%3CCAFRjkg6OfuVm9h58oNS_On6jN1LmYXaSn7TFsJmOiX-YTbBSqg%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002366.html">
   <LINK REL="Next"  HREF="002368.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Sorcerer-admins] Sorcerer common install-script</H1>
    <B>don dondon</B> 
    <A HREF="mailto:sorcerer-admins%40lists.berlios.de?Subject=Re%3A%20%5BSorcerer-admins%5D%20Sorcerer%20common%20install-script&In-Reply-To=%3CCAFRjkg6OfuVm9h58oNS_On6jN1LmYXaSn7TFsJmOiX-YTbBSqg%40mail.gmail.com%3E"
       TITLE="[Sorcerer-admins] Sorcerer common install-script">psykner at gmail.com
       </A><BR>
    <I>Tue Aug 14 15:52:03 CEST 2012</I>
    <P><UL>
        <LI>Previous message: <A HREF="002366.html">[Sorcerer-admins] cgroup modified and explained...
</A></li>
        <LI>Next message: <A HREF="002368.html">[Sorcerer-admins] Sorcerer common install-script
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2367">[ date ]</a>
              <a href="thread.html#2367">[ thread ]</a>
              <a href="subject.html#2367">[ subject ]</a>
              <a href="author.html#2367">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE> Hi there
 I'm sitting here whiching I knew how to install Sorcerer, but I
haven't got the skill at the moment. That is why I would love if there
was an Install-script that could install the Kernel and the Sorcerer
extensions on any computer, at least on Lenovo-Thinkpad. If you put
such a script on your HP, you would sertently get many more users for
Sorcerer.
 I'm not quite aquanted with how Sorcerer works but, If you gave the
Cast-script the abillity to find a program's source from the users
input (cast &lt;program-name&gt;) and gave the user a possibility, via
numbers, to choose the right program from the ones Cast found on the
net, it would be very popular.
 Write back if you want a better explanation.

 Peter

2012/8/14  &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">sorcerer-admins-request at lists.berlios.de</A>&gt;:
&gt;<i> Send Sorcerer-admins mailing list submissions to
</I>&gt;<i>         <A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">sorcerer-admins at lists.berlios.de</A>
</I>&gt;<i>
</I>&gt;<i> To subscribe or unsubscribe via the World Wide Web, visit
</I>&gt;<i>         <A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">https://lists.berlios.de/mailman/listinfo/sorcerer-admins</A>
</I>&gt;<i> or, via email, send a message with subject or body 'help' to
</I>&gt;<i>         <A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">sorcerer-admins-request at lists.berlios.de</A>
</I>&gt;<i>
</I>&gt;<i> You can reach the person managing the list at
</I>&gt;<i>         <A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">sorcerer-admins-owner at lists.berlios.de</A>
</I>&gt;<i>
</I>&gt;<i> When replying, please edit your Subject line so it is more specific
</I>&gt;<i> than &quot;Re: Contents of Sorcerer-admins digest...&quot;
</I>&gt;<i>
</I>&gt;<i> Today's Topics:
</I>&gt;<i>
</I>&gt;<i>    1. Cosmetic changes... (Kyle Sallee)
</I>&gt;<i>    2. Re: Cosmetic changes... (Evert Vorster)
</I>&gt;<i>    3. cgroup modified and explained... (Kyle Sallee)
</I>&gt;<i>    4. Re: cgroup modified and explained... (Evert Vorster)
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> ---------- Videresendt meddelelse ----------
</I>&gt;<i> From: Kyle Sallee &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">kyle.sallee at gmail.com</A>&gt;
</I>&gt;<i> To: Sorcerer List &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">sorcerer-admins at lists.berlios.de</A>&gt;
</I>&gt;<i> Cc:
</I>&gt;<i> Date: Mon, 13 Aug 2012 09:28:48 -0700
</I>&gt;<i> Subject: [Sorcerer-admins] Cosmetic changes...
</I>&gt;<i> # /etc/init.d/cgroup configure
</I>&gt;<i>
</I>&gt;<i> Now works more as intended.
</I>&gt;<i> It can create a cgroup at any level.
</I>&gt;<i> And values can be adjusted at any level.
</I>&gt;<i>
</I>&gt;<i> The previous cpufreq_{performance,powersave,userspace,ondemand,conservative}
</I>&gt;<i> and such
</I>&gt;<i> are now a single program called &quot;cpufreq.&quot;
</I>&gt;<i>
</I>&gt;<i> Example
</I>&gt;<i> $ cpufreq ondemand
</I>&gt;<i> The line changes governors for all processors to ondemand.
</I>&gt;<i>
</I>&gt;<i> For convenience bash completion is provided.
</I>&gt;<i>
</I>&gt;<i> $ cpufreq o[tab]
</I>&gt;<i> That will complete to
</I>&gt;<i> $ cpufreq ondemand
</I>&gt;<i>
</I>&gt;<i> Likewise if the first parameter is userspace
</I>&gt;<i> then tab completion on the second parameter
</I>&gt;<i> provide acceptable supported frequencies.
</I>&gt;<i>
</I>&gt;<i> Hopefully, this will be equally convenient
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> ---------- Videresendt meddelelse ----------
</I>&gt;<i> From: Evert Vorster &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">evorster at gmail.com</A>&gt;
</I>&gt;<i> To: <A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">sorcerer-admins at lists.berlios.de</A>
</I>&gt;<i> Cc:
</I>&gt;<i> Date: Mon, 13 Aug 2012 18:43:18 +0100
</I>&gt;<i> Subject: Re: [Sorcerer-admins] Cosmetic changes...
</I>&gt;<i> Sounds nice!
</I>&gt;<i>
</I>&gt;<i> Once I am not so very busy at work, I will take a look at the new and improved cgroup.
</I>&gt;<i> I'm happy with ondemand as a default cpu frequency governor. Gives me speed when I need it, and idles when I don't.
</I>&gt;<i>
</I>&gt;<i> -Evert-
</I>&gt;<i>
</I>&gt;<i> On 13 August 2012 17:28, Kyle Sallee &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">kyle.sallee at gmail.com</A>&gt; wrote:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> # /etc/init.d/cgroup configure
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Now works more as intended.
</I>&gt;&gt;<i> It can create a cgroup at any level.
</I>&gt;&gt;<i> And values can be adjusted at any level.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> The previous cpufreq_{performance,powersave,userspace,ondemand,conservative}
</I>&gt;&gt;<i> and such
</I>&gt;&gt;<i> are now a single program called &quot;cpufreq.&quot;
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Example
</I>&gt;&gt;<i> $ cpufreq ondemand
</I>&gt;&gt;<i> The line changes governors for all processors to ondemand.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> For convenience bash completion is provided.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> $ cpufreq o[tab]
</I>&gt;&gt;<i> That will complete to
</I>&gt;&gt;<i> $ cpufreq ondemand
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Likewise if the first parameter is userspace
</I>&gt;&gt;<i> then tab completion on the second parameter
</I>&gt;&gt;<i> provide acceptable supported frequencies.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Hopefully, this will be equally convenient
</I>&gt;&gt;<i> _______________________________________________
</I>&gt;&gt;<i> Sorcerer-admins mailing list
</I>&gt;&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">Sorcerer-admins at lists.berlios.de</A>
</I>&gt;&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">https://lists.berlios.de/mailman/listinfo/sorcerer-admins</A>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> --
</I>&gt;<i> Those who do not understand UNIX are doomed to reinvent it, poorly.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> ---------- Videresendt meddelelse ----------
</I>&gt;<i> From: Kyle Sallee &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">kyle.sallee at gmail.com</A>&gt;
</I>&gt;<i> To: Sorcerer List &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">sorcerer-admins at lists.berlios.de</A>&gt;
</I>&gt;<i> Cc:
</I>&gt;<i> Date: Mon, 13 Aug 2012 12:40:38 -0700
</I>&gt;<i> Subject: [Sorcerer-admins] cgroup modified and explained...
</I>&gt;<i> I modified cgroup to be a bit more like cpufreq.
</I>&gt;<i> Therefore, cgroup-{affine,fast,idle,slow} were removed in favor of
</I>&gt;<i> /usr/bin/cgroup
</I>&gt;<i> It accepts parameters and provides bash completion.
</I>&gt;<i> Additionally, I checked to ensure that all aspects of it work properly.
</I>&gt;<i> And finally, they do.
</I>&gt;<i> Previously, there were some ownership/permission problems.
</I>&gt;<i>
</I>&gt;<i> This change along with the aforementioned change to cpufreq
</I>&gt;<i> will be available in the next UTC 00:00 grimoire.
</I>&gt;<i> And since most boxes typically lag a day behind,
</I>&gt;<i> expect to be able to enjoy the benefits no sooner than about 30 hours from now.
</I>&gt;<i>
</I>&gt;<i> Basically, one of the final set of changes before releasing new I/Rs
</I>&gt;<i> is to revise implementations that have been awaiting revision
</I>&gt;<i> and to hunt bugs, almost any bugs.
</I>&gt;<i> That way the next I/Rs provide maximum functionality and minimal annoyance.
</I>&gt;<i>
</I>&gt;<i> For those who have not used the cgroup helper program
</I>&gt;<i> it combines a bit of the functionality
</I>&gt;<i> that was previously provided by nice and taskset
</I>&gt;<i> on a purely POSIX box without cgroups enabled.
</I>&gt;<i>
</I>&gt;<i> However, on a cgroup box, nice does not work exactly as expected
</I>&gt;<i> and that is the reason for the existence of top level control groups:
</I>&gt;<i> fast; norm; slow; idle.
</I>&gt;<i> By default users will not have access to fast unless enabled.
</I>&gt;<i> However on a quad core box user cgroups will look something like
</I>&gt;<i>
</I>&gt;<i> /sys/fs/cgroup/{norm,slow,idle}/user/$USER/{0,1,2,3}
</I>&gt;<i>
</I>&gt;<i> When a user logs in via ssh or kdm, gdm, xdm
</I>&gt;<i> then the user's processes run in
</I>&gt;<i> /sys/fs/cgroup/norm/user/$USER
</I>&gt;<i> However, transition from /sys/fs/group
</I>&gt;<i> for user owned processes
</I>&gt;<i> is provided when a user logs in via getty.
</I>&gt;<i>
</I>&gt;<i> Let us say that the user wants his virtual server to run slower
</I>&gt;<i> $ cpufreq slow $( pgrep qemu )
</I>&gt;<i> Easy yes?
</I>&gt;<i> What if the user wants to xz compress a file, but for minimal processing usage?
</I>&gt;<i> $ xz huge.tar &amp;
</I>&gt;<i> $ cpufreq idle $( pgrep -u $USER xz )
</I>&gt;<i> And if the user also wants xz to run on a specific processor such as
</I>&gt;<i> the last one?
</I>&gt;<i> $ cpufreq affine 3 $( pgrep -u $USER xz )
</I>&gt;<i>
</I>&gt;<i> Easy.
</I>&gt;<i> But of course the user can
</I>&gt;<i> always echo user owned PIDs
</I>&gt;<i> to the tasks files owned by the user.
</I>&gt;<i> Therefore, the use of /usr/bin/cgroup
</I>&gt;<i> is optional for a SA that already understands
</I>&gt;<i> the use of control groups and the default implementation on Sorcerer.
</I>&gt;<i>
</I>&gt;<i> If expecting to use maximum processing power for a long duration on a
</I>&gt;<i> multi-user box
</I>&gt;<i> then moving one's own processes to the slow or idle hierarchy of control groups
</I>&gt;<i> allows those processes to defer processing power to other processing
</I>&gt;<i> running on the box.
</I>&gt;<i> Doing so would be considered polite.
</I>&gt;<i>
</I>&gt;<i> Someone might yet wonder what happens
</I>&gt;<i> if a user starts xz in the above example and
</I>&gt;<i> then executes:
</I>&gt;<i> $ renice +19 $( pgrep -u $USER xz )
</I>&gt;<i>
</I>&gt;<i> The answer that if the scheduler will prefer to allocate
</I>&gt;<i> more processing power to the other processes that
</I>&gt;<i> are in the same control group as xz.
</I>&gt;<i> Therefore, if processing power is in short supply
</I>&gt;<i> then xz's share will be diminished relative to other
</I>&gt;<i> processes within the same control group.
</I>&gt;<i>
</I>&gt;<i> Therefore, if users bill jane and john are competing
</I>&gt;<i> for an insufficient amount of processing power.
</I>&gt;<i> And each is running their processes from
</I>&gt;<i> /sys/fs/cgroup/user/{bill,jane,john}, respectively
</I>&gt;<i> then each user will receive approximately an equal portion of processing power.
</I>&gt;<i> If bill nices his xz process then bill still receives approximately
</I>&gt;<i> 1/3rd total for his processes.
</I>&gt;<i> However, if bill moves xz's PID into /sys/fs/cgroup/slow/user/bill/tasks
</I>&gt;<i> then bill's processes in /sys/fs/cgroup/norm/user/bill/tasks will run faster.
</I>&gt;<i> However, his xz process will run much slower than previously.
</I>&gt;<i>
</I>&gt;<i> Perhaps the above explanation might seem confusing?
</I>&gt;<i> However, it is a very good implementation for directing processing power
</I>&gt;<i> for both uniprocess or SMP boxes regardless of whether the box has one
</I>&gt;<i> or more concurrent users.
</I>&gt;<i> And additionally, the default implementation should prevent
</I>&gt;<i> the condition of X starving for processing power.
</I>&gt;<i> Therefore, a number of tasks that contribute to a high load
</I>&gt;<i> should not impact the visual performance of X.
</I>&gt;<i> However, in extreme circumstances the user's firefox or video player
</I>&gt;<i> might be starving.
</I>&gt;<i> And that situation can be solved by using &quot;nice&quot; or the &quot;cgroup&quot;
</I>&gt;<i> utility as described above.
</I>&gt;<i>
</I>&gt;<i> Most system started services will run in cgroups such as:
</I>&gt;<i> /sys/fs/cgroup/norm/sys/$SERVICE
</I>&gt;<i> where $SERVICE is the name of the service.
</I>&gt;<i> However, some variation exists in order to make some services receive preferred
</I>&gt;<i> or deferred scheduling of processing power.
</I>&gt;<i> For example, gdm, kdm, xdm will run from
</I>&gt;<i> /sys/fs/cgroup/fast/sys/{gdm,kdm,xdm}, respectively.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> For those who prefer a more simple to understand implementation that
</I>&gt;<i> works more like POSIX
</I>&gt;<i> then rejoice because Evert is designing that implementation.
</I>&gt;<i> However, lament because I am coding it, so it might have bugs.  :)
</I>&gt;<i> Evert wants for all user owned processes to always run in /sys/fs/cgroup/tasks
</I>&gt;<i> Additionally, Evert wants for system started services to run in
</I>&gt;<i> /sys/fs/cgroup/$SERVICE
</I>&gt;<i> Succinctly stated, it is an implementation that sacrifices the
</I>&gt;<i> benefits of hierarchy.
</I>&gt;<i>
</I>&gt;<i> Also it does not install a /usr/bin/cgroup to assist users.
</I>&gt;<i> Users will control allocation of processing power using the &quot;nice&quot; and
</I>&gt;<i> &quot;taskset&quot; utilities.
</I>&gt;<i> And it will not maintain a fair allocation of processing power among users.
</I>&gt;<i> If john has 5 processes running full bore, and jane has one process
</I>&gt;<i> running full bore
</I>&gt;<i> then john will receive 5 times more processing power than jane.
</I>&gt;<i> It is old style where user with the most full bore processes wins.
</I>&gt;<i> Controlling the allocation of processing power among services
</I>&gt;<i> is not accomplished with nice but instead by changing
</I>&gt;<i> the value of cpu.shares for one or more service cgroups.
</I>&gt;<i>
</I>&gt;<i> After the implementation is debugged, tested, honed, revised, etc...
</I>&gt;<i> then it will be available from the spells {init-functions,cgroup}-evert
</I>&gt;<i> Until then those spells will retain the broke attribute.
</I>&gt;<i>
</I>&gt;<i> Anyone with an opinion about the default cgroup implementation or Evert's
</I>&gt;<i> is welcome to state reasons, observations, tests on the email list.
</I>&gt;<i> If we can use that information to make the cgroup implementation better
</I>&gt;<i> then we will.
</I>&gt;<i>
</I>&gt;<i> I might have already stated in a previous message,
</I>&gt;<i> but if I forgot then please be aware that
</I>&gt;<i> # /etc/init.d/cgroup configure
</I>&gt;<i> has been improved.
</I>&gt;<i> It now allows creation of a cgroup at any level instead of merely top level.
</I>&gt;<i> And it allows editing of controls in any control group.
</I>&gt;<i> And it can display the control groups that are defined and initialized at boot.
</I>&gt;<i> And the newest ability is to select which subsystems are enabled
</I>&gt;<i> when the cgroup file system is mounted on /sys/fs/cgroup/
</I>&gt;<i>
</I>&gt;<i> For example, why bother mounting the cpuset subsystem on a uniprocessor box?
</I>&gt;<i> And the default cgroup implementation no longer mounts the blkio subsystem
</I>&gt;<i> because I changed the default io scheduler from CFQ back to deadline.
</I>&gt;<i> Therefore, if a SA wants to use blkio then the blkio subsystem can be enabled
</I>&gt;<i> and the default scheduler can be selected per device or at boot by
</I>&gt;<i> kernel parameter.
</I>&gt;<i> And those with no concern for controlling memory and who do not install
</I>&gt;<i> immune-sentinel-memory
</I>&gt;<i> can for example disable the memory subsystem.
</I>&gt;<i> Those who want to use &quot;nice&quot; exclusively for scheduling processing power,
</I>&gt;<i> and still want the default cgroup implementation could choose to
</I>&gt;<i> mount the cgroup file system without the cpu subsystem.
</I>&gt;<i> The possibilities are plentiful.
</I>&gt;<i>
</I>&gt;<i> I hope that everyone will feel satisfied with the default implementation
</I>&gt;<i> or be able to suitably modify it in order to achieve desired
</I>&gt;<i> performance and functionality.
</I>&gt;<i> Of course dissatisfaction can impel change and possibly improvement.
</I>&gt;<i> Therefore, please also provide suggestions
</I>&gt;<i> for ways in which the functionality can be improved.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> ---------- Videresendt meddelelse ----------
</I>&gt;<i> From: Evert Vorster &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">evorster at gmail.com</A>&gt;
</I>&gt;<i> To: <A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">sorcerer-admins at lists.berlios.de</A>
</I>&gt;<i> Cc:
</I>&gt;<i> Date: Tue, 14 Aug 2012 02:30:18 +0100
</I>&gt;<i> Subject: Re: [Sorcerer-admins] cgroup modified and explained...
</I>&gt;<i> Let me be the first to say that the default settings for cgroup are very fine indeed for the largest possible set of system configs.
</I>&gt;<i>
</I>&gt;<i> I will debug the alternate settings for cgroup, but that is really only to be used on a single user system, and will be tuned as such, and users running the alternate cgroup settings on servers or with many users will run into issues that simply does not happen on the default setup.
</I>&gt;<i>
</I>&gt;<i> Happy hacking!
</I>&gt;<i> -Evert-
</I>&gt;<i>
</I>&gt;<i> On 13 August 2012 20:40, Kyle Sallee &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">kyle.sallee at gmail.com</A>&gt; wrote:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> I modified cgroup to be a bit more like cpufreq.
</I>&gt;&gt;<i> Therefore, cgroup-{affine,fast,idle,slow} were removed in favor of
</I>&gt;&gt;<i> /usr/bin/cgroup
</I>&gt;&gt;<i> It accepts parameters and provides bash completion.
</I>&gt;&gt;<i> Additionally, I checked to ensure that all aspects of it work properly.
</I>&gt;&gt;<i> And finally, they do.
</I>&gt;&gt;<i> Previously, there were some ownership/permission problems.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> This change along with the aforementioned change to cpufreq
</I>&gt;&gt;<i> will be available in the next UTC 00:00 grimoire.
</I>&gt;&gt;<i> And since most boxes typically lag a day behind,
</I>&gt;&gt;<i> expect to be able to enjoy the benefits no sooner than about 30 hours from now.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Basically, one of the final set of changes before releasing new I/Rs
</I>&gt;&gt;<i> is to revise implementations that have been awaiting revision
</I>&gt;&gt;<i> and to hunt bugs, almost any bugs.
</I>&gt;&gt;<i> That way the next I/Rs provide maximum functionality and minimal annoyance.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> For those who have not used the cgroup helper program
</I>&gt;&gt;<i> it combines a bit of the functionality
</I>&gt;&gt;<i> that was previously provided by nice and taskset
</I>&gt;&gt;<i> on a purely POSIX box without cgroups enabled.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> However, on a cgroup box, nice does not work exactly as expected
</I>&gt;&gt;<i> and that is the reason for the existence of top level control groups:
</I>&gt;&gt;<i> fast; norm; slow; idle.
</I>&gt;&gt;<i> By default users will not have access to fast unless enabled.
</I>&gt;&gt;<i> However on a quad core box user cgroups will look something like
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> /sys/fs/cgroup/{norm,slow,idle}/user/$USER/{0,1,2,3}
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> When a user logs in via ssh or kdm, gdm, xdm
</I>&gt;&gt;<i> then the user's processes run in
</I>&gt;&gt;<i> /sys/fs/cgroup/norm/user/$USER
</I>&gt;&gt;<i> However, transition from /sys/fs/group
</I>&gt;&gt;<i> for user owned processes
</I>&gt;&gt;<i> is provided when a user logs in via getty.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Let us say that the user wants his virtual server to run slower
</I>&gt;&gt;<i> $ cpufreq slow $( pgrep qemu )
</I>&gt;&gt;<i> Easy yes?
</I>&gt;&gt;<i> What if the user wants to xz compress a file, but for minimal processing usage?
</I>&gt;&gt;<i> $ xz huge.tar &amp;
</I>&gt;&gt;<i> $ cpufreq idle $( pgrep -u $USER xz )
</I>&gt;&gt;<i> And if the user also wants xz to run on a specific processor such as
</I>&gt;&gt;<i> the last one?
</I>&gt;&gt;<i> $ cpufreq affine 3 $( pgrep -u $USER xz )
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Easy.
</I>&gt;&gt;<i> But of course the user can
</I>&gt;&gt;<i> always echo user owned PIDs
</I>&gt;&gt;<i> to the tasks files owned by the user.
</I>&gt;&gt;<i> Therefore, the use of /usr/bin/cgroup
</I>&gt;&gt;<i> is optional for a SA that already understands
</I>&gt;&gt;<i> the use of control groups and the default implementation on Sorcerer.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> If expecting to use maximum processing power for a long duration on a
</I>&gt;&gt;<i> multi-user box
</I>&gt;&gt;<i> then moving one's own processes to the slow or idle hierarchy of control groups
</I>&gt;&gt;<i> allows those processes to defer processing power to other processing
</I>&gt;&gt;<i> running on the box.
</I>&gt;&gt;<i> Doing so would be considered polite.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Someone might yet wonder what happens
</I>&gt;&gt;<i> if a user starts xz in the above example and
</I>&gt;&gt;<i> then executes:
</I>&gt;&gt;<i> $ renice +19 $( pgrep -u $USER xz )
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> The answer that if the scheduler will prefer to allocate
</I>&gt;&gt;<i> more processing power to the other processes that
</I>&gt;&gt;<i> are in the same control group as xz.
</I>&gt;&gt;<i> Therefore, if processing power is in short supply
</I>&gt;&gt;<i> then xz's share will be diminished relative to other
</I>&gt;&gt;<i> processes within the same control group.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Therefore, if users bill jane and john are competing
</I>&gt;&gt;<i> for an insufficient amount of processing power.
</I>&gt;&gt;<i> And each is running their processes from
</I>&gt;&gt;<i> /sys/fs/cgroup/user/{bill,jane,john}, respectively
</I>&gt;&gt;<i> then each user will receive approximately an equal portion of processing power.
</I>&gt;&gt;<i> If bill nices his xz process then bill still receives approximately
</I>&gt;&gt;<i> 1/3rd total for his processes.
</I>&gt;&gt;<i> However, if bill moves xz's PID into /sys/fs/cgroup/slow/user/bill/tasks
</I>&gt;&gt;<i> then bill's processes in /sys/fs/cgroup/norm/user/bill/tasks will run faster.
</I>&gt;&gt;<i> However, his xz process will run much slower than previously.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Perhaps the above explanation might seem confusing?
</I>&gt;&gt;<i> However, it is a very good implementation for directing processing power
</I>&gt;&gt;<i> for both uniprocess or SMP boxes regardless of whether the box has one
</I>&gt;&gt;<i> or more concurrent users.
</I>&gt;&gt;<i> And additionally, the default implementation should prevent
</I>&gt;&gt;<i> the condition of X starving for processing power.
</I>&gt;&gt;<i> Therefore, a number of tasks that contribute to a high load
</I>&gt;&gt;<i> should not impact the visual performance of X.
</I>&gt;&gt;<i> However, in extreme circumstances the user's firefox or video player
</I>&gt;&gt;<i> might be starving.
</I>&gt;&gt;<i> And that situation can be solved by using &quot;nice&quot; or the &quot;cgroup&quot;
</I>&gt;&gt;<i> utility as described above.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Most system started services will run in cgroups such as:
</I>&gt;&gt;<i> /sys/fs/cgroup/norm/sys/$SERVICE
</I>&gt;&gt;<i> where $SERVICE is the name of the service.
</I>&gt;&gt;<i> However, some variation exists in order to make some services receive preferred
</I>&gt;&gt;<i> or deferred scheduling of processing power.
</I>&gt;&gt;<i> For example, gdm, kdm, xdm will run from
</I>&gt;&gt;<i> /sys/fs/cgroup/fast/sys/{gdm,kdm,xdm}, respectively.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> For those who prefer a more simple to understand implementation that
</I>&gt;&gt;<i> works more like POSIX
</I>&gt;&gt;<i> then rejoice because Evert is designing that implementation.
</I>&gt;&gt;<i> However, lament because I am coding it, so it might have bugs.  :)
</I>&gt;&gt;<i> Evert wants for all user owned processes to always run in /sys/fs/cgroup/tasks
</I>&gt;&gt;<i> Additionally, Evert wants for system started services to run in
</I>&gt;&gt;<i> /sys/fs/cgroup/$SERVICE
</I>&gt;&gt;<i> Succinctly stated, it is an implementation that sacrifices the
</I>&gt;&gt;<i> benefits of hierarchy.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Also it does not install a /usr/bin/cgroup to assist users.
</I>&gt;&gt;<i> Users will control allocation of processing power using the &quot;nice&quot; and
</I>&gt;&gt;<i> &quot;taskset&quot; utilities.
</I>&gt;&gt;<i> And it will not maintain a fair allocation of processing power among users.
</I>&gt;&gt;<i> If john has 5 processes running full bore, and jane has one process
</I>&gt;&gt;<i> running full bore
</I>&gt;&gt;<i> then john will receive 5 times more processing power than jane.
</I>&gt;&gt;<i> It is old style where user with the most full bore processes wins.
</I>&gt;&gt;<i> Controlling the allocation of processing power among services
</I>&gt;&gt;<i> is not accomplished with nice but instead by changing
</I>&gt;&gt;<i> the value of cpu.shares for one or more service cgroups.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> After the implementation is debugged, tested, honed, revised, etc...
</I>&gt;&gt;<i> then it will be available from the spells {init-functions,cgroup}-evert
</I>&gt;&gt;<i> Until then those spells will retain the broke attribute.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Anyone with an opinion about the default cgroup implementation or Evert's
</I>&gt;&gt;<i> is welcome to state reasons, observations, tests on the email list.
</I>&gt;&gt;<i> If we can use that information to make the cgroup implementation better
</I>&gt;&gt;<i> then we will.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> I might have already stated in a previous message,
</I>&gt;&gt;<i> but if I forgot then please be aware that
</I>&gt;&gt;<i> # /etc/init.d/cgroup configure
</I>&gt;&gt;<i> has been improved.
</I>&gt;&gt;<i> It now allows creation of a cgroup at any level instead of merely top level.
</I>&gt;&gt;<i> And it allows editing of controls in any control group.
</I>&gt;&gt;<i> And it can display the control groups that are defined and initialized at boot.
</I>&gt;&gt;<i> And the newest ability is to select which subsystems are enabled
</I>&gt;&gt;<i> when the cgroup file system is mounted on /sys/fs/cgroup/
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> For example, why bother mounting the cpuset subsystem on a uniprocessor box?
</I>&gt;&gt;<i> And the default cgroup implementation no longer mounts the blkio subsystem
</I>&gt;&gt;<i> because I changed the default io scheduler from CFQ back to deadline.
</I>&gt;&gt;<i> Therefore, if a SA wants to use blkio then the blkio subsystem can be enabled
</I>&gt;&gt;<i> and the default scheduler can be selected per device or at boot by
</I>&gt;&gt;<i> kernel parameter.
</I>&gt;&gt;<i> And those with no concern for controlling memory and who do not install
</I>&gt;&gt;<i> immune-sentinel-memory
</I>&gt;&gt;<i> can for example disable the memory subsystem.
</I>&gt;&gt;<i> Those who want to use &quot;nice&quot; exclusively for scheduling processing power,
</I>&gt;&gt;<i> and still want the default cgroup implementation could choose to
</I>&gt;&gt;<i> mount the cgroup file system without the cpu subsystem.
</I>&gt;&gt;<i> The possibilities are plentiful.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> I hope that everyone will feel satisfied with the default implementation
</I>&gt;&gt;<i> or be able to suitably modify it in order to achieve desired
</I>&gt;&gt;<i> performance and functionality.
</I>&gt;&gt;<i> Of course dissatisfaction can impel change and possibly improvement.
</I>&gt;&gt;<i> Therefore, please also provide suggestions
</I>&gt;&gt;<i> for ways in which the functionality can be improved.
</I>&gt;&gt;<i> _______________________________________________
</I>&gt;&gt;<i> Sorcerer-admins mailing list
</I>&gt;&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">Sorcerer-admins at lists.berlios.de</A>
</I>&gt;&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">https://lists.berlios.de/mailman/listinfo/sorcerer-admins</A>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> --
</I>&gt;<i> Those who do not understand UNIX are doomed to reinvent it, poorly.
</I>&gt;<i>
</I>&gt;<i> _______________________________________________
</I>&gt;<i> Sorcerer-admins mailing list
</I>&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">Sorcerer-admins at lists.berlios.de</A>
</I>&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">https://lists.berlios.de/mailman/listinfo/sorcerer-admins</A>
</I>


-- 
 Mvh.
-----------------------------
 Peter Sloth
 4010 7200
 <A HREF="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">psykner at gmail.com</A>
</PRE>


<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002366.html">[Sorcerer-admins] cgroup modified and explained...
</A></li>
	<LI>Next message: <A HREF="002368.html">[Sorcerer-admins] Sorcerer common install-script
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2367">[ date ]</a>
              <a href="thread.html#2367">[ thread ]</a>
              <a href="subject.html#2367">[ subject ]</a>
              <a href="author.html#2367">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/sorcerer-admins">More information about the Sorcerer-admins
mailing list</a><br>
</body></html>
